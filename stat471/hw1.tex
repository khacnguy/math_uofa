\documentclass[11pt]{report}
    \title{\textbf{Math 217 Homework I}}
    \author{Khac Nguyen Nguyen}
    \date{}
    
    \addtolength{\topmargin}{-3cm}
    \addtolength{\textheight}{3cm}
    
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{xfrac}
\usepackage{hyperref}

\usepgfplotslibrary{polar}
\usepgflibrary{shapes.geometric}
\usetikzlibrary{calc}
\pgfplotsset{compat = newest}
\pgfplotsset{my style/.append style = {axis x line = middle, axis y line = middle, xlabel={$x$}, ylabel={$y$}, axis equal}}
\begin{document}
\tableofcontents
\newpage
\chapter{HW1}
\section{}
Without loss of generality, we can assume dim(Image$(g)) = 1$.
Assume that $g$ is only defined in the set $[-N,N] \times [-M,M]$ where $N,M$ are arbitary. \\
We have that 
\[
    \int_{[-N,N] \times [-M,M]} g(x,y) dF(x,y) = \lim_{n,m \to \infty} \sum_{i=1}^n \sum_{j=1}^m g(x^*, y^*) \Delta F_{i,j}
\]
where with $x_i = \frac{-(n-i)N + iN}{n} = \frac{2iN - nN}{n}$, $y_i = \frac{-(m-i)M + iM}{m} = \frac{2iM - mM}{m}$, we have
\begin{equation*}
    \begin{aligned}
        \Delta F_{i,j} &= F(x_i, y_j) - F(x_{i-1}, y_j) - F(x_i, y_{j-1}) + F(x_{i-1}, y_{j-1}) \\
        &= \int_{x_{i-1}}^{x_i} \int_{y_{j-1}}^{y_j} f(x,y) dxdy
    \end{aligned}
\end{equation*}
Hence, 
\[
    \int_{[-N,N] \times [-M,M]} g(x,y) dF(x,y) = \lim_{n,m \to \infty} \sum_{i=1}^n \sum_{j=1}^m g(x^*, y^*) \int_{x_{i-1}}^{x_i} \int_{y_{j-1}}^{y_j} f(x,y) dxdy
\]
\begin{equation*}
    \begin{aligned}
        E[g(X,Y)] &= \int_{[-N,N] \times [-M,M]} g(x,y) f(x,y) dxdy \\
        &=  \lim_{n,m \to \infty} \sum_{i=1}^n \sum_{j=1}^m \int_{x_{i-1}}^{x_i} \int_{y_{j-1}}^{y_j} g(x, y) f(x,y) dxdy 
    \end{aligned}
\end{equation*}
We know that $g$ is uniformly continuous as it is continuous in a compact set, we have that with large enough n,m
\[
    | g(x,y) - g(x^*, y^*) | < \epsilon
\]
Hence, 
\begin{equation*}
    \begin{aligned}
        & \left| \int_{[-N,N] \times [-M,M]} g(x,y) dF(x,y)  - E[g(X,Y)] \right| \\
        =&  \left| \lim_{n,m \to \infty} \sum_{i=1}^n \sum_{j=1}^m  \int_{x_{i-1}}^{x_i} \int_{y_{j-1}}^{y_j} (g(x, y) - g(x^*,y^*)) f(x,y)  dxdy \right| \\
        <& \left| \lim_{n,m \to \infty} \sum_{i=1}^n \sum_{j=1}^m  \int_{x_{i-1}}^{x_i} \int_{y_{j-1}}^{y_j} \epsilon f(x,y)  dxdy \right| \\
        =& \epsilon \left|\lim_{n,m \to \infty} \sum_{i=1}^n \sum_{j=1}^m  \int_{x_{i-1}}^{x_i} \int_{y_{j-1}}^{y_j} f(x,y)  dxdy \right| \\
        =& \epsilon
    \end{aligned}
\end{equation*}
Since, $\epsilon$ is arbitary, $\int_{[-N,N] \times [-M,M]} g(x,y) dF(x,y)  = E[g(X,Y)]$, and since $N,M$ are arbitary,
\[
    \int_{\mathbb{R}^2} g(x,y) dF(x,y)  = E[g(X,Y)]
\]
\pagebreak
\section{}
Because $s_i > a_i$ for all $i$, $X(0)$ does not affect stationary distributions.
\begin{equation*}
    \begin{aligned}
        \pi(0) &= \left(1 + \frac{a_0}{s_1} + \frac{a_0 a_1}{s_1 s_2} + \hdots \right)^{-1} \\
        &= \left(1 + \frac{1}{3} + \frac{1 \cdot 2}{3 \cdot 4} + \frac{1 \cdot 2 \cdot 3}{3 \cdot 4 \cdot 5} + \hdots\right)^{-1} \\
        &= \left(1 + \sum_{i=1}^\infty \frac{2 \cdot i!}{(i+2)!} \right)^{-1} \\
        &= \left(1 + \sum_{i=1}^\infty \frac{2}{(i+1)(i+2)} \right)^{-1} \\
        &= \left(1 + 2 \sum_{i=1}^\infty \left(\frac{1}{i+1} - \frac{1}{i+2} \right)\right)^{-1} \\
        &= \left(1 + 2 \left(\frac{1}{2}\right) \right)^{-1} \\
        &= \frac{1}{2}
    \end{aligned}
\end{equation*}
Hence, we can calculate 
\[
    \pi(i) = \pi(0) \frac{a_0 a_1 \hdots a_{i-1}}{s_1 s_2 \hdots s_i} = \frac{1}{2} \cdot \frac{2 \cdot i!}{(i+2)!} = \frac{1}{(i+1)(i+2)}
\]
\newpage
\section{}
\subsection{}
\[
    f^{-1}(\varnothing) = \{x\in S: f(x) \in \varnothing\} = \varnothing
\]
\subsection{}
\[
    f^{-1}(B^C) = \{x\in S: f(x) \notin B\} = S \backslash \{x\in S: f(x) \in B\} = S \backslash (f^{-1}(B)) = f^{-1}(B)^C
\]
\subsection{}
\begin{equation*}
    \begin{aligned}
        f^{-1} \left(\bigcap_\beta B_\beta \right) &= \{x \in S: f(x) \in \bigcap_\beta B_\beta \} \\
        &= \bigcap_\beta \{x \in S: f(x) \in B_\beta\} \\
        &= \bigcap_\beta f^{-1}(B_\beta)
    \end{aligned}
\end{equation*}
\subsection{}
\begin{equation*}
    \begin{aligned}
        f^{-1} \left(\bigcup_\beta B_\beta \right) &= \{x \in S: f(x) \in \bigcup_\beta B_\beta \} \\
        &= \bigcup_\beta \{x \in S: f(x) \in B_\beta\} \\
        &= \bigcup_\beta f^{-1}(B_\beta) 
    \end{aligned}
\end{equation*}
\newpage
\section{}
Let $\{Y_i\}_{i=1}^N$ be geometric distributions, which is also a filtration. We will use the geometric distributions to estimate $\sin(X \ln(X))$, 
where $X$ is a $(2,1/2)$-negative binomial.  
\[
    \alpha_i = \frac{p(Y_i)}{q(Y_i)} = \frac{\begin{pmatrix} Y_i+1 \\ Y_i \end{pmatrix} \left(\frac{1}{2}\right)^2 \left( \frac{1}{2}\right)^{Y_i-2}}{\frac{1}{2} \left( \frac{1}{2}\right)^{Y_i-1}} = Y_i + 1
\]
and with
\[
    L_n = \prod_{i=1}^n \alpha_i = \prod_{i=1}^n (Y_i + 1)
\]
We have
\begin{equation*}
    \begin{aligned}
        &E[L_n g(Y_1,Y_2, \hdots, Y_n)] \\
        =& E[ E[L_n | \mathcal{F}_n] g(Y_1, Y_2, \hdots, Y_n)] \\
        =& E\left[\prod_{i=1}^n \frac{p(Y_i)}{q(Y_i)} g(Y_1,Y_2, \hdots, Y_n) \right] \\
        =& \sum_{j_1, j_2, \hdots, j_n = 1}^\infty \prod_{i=1}^n \frac{p(j_i)}{q(j_i)} g(j_1, j_2, \hdots, j_n) q(j_1) q(j_2) \hdots q(j_n) \\
        =& \sum_{j_1, j_2, \hdots, j_n = 1}^\infty g(j_1, j_2, \hdots, j_n) p(j_1)p(j_2)\hdots p(j_n) \\
        =& E[g(X_1, X_2, \hdots, X_n)]
    \end{aligned}
\end{equation*}
where $X_i$ is a $(2,1/2)$-negative binomial.
Therefore, to estimate $E[\sin(X\ln(X))]$, calculate
\[
    g(X_1, X_2, \hdots, X_n) = \prod_{i=1}^n (Y_i+1) g(Y_1, Y_2, \hdots, Y_n)
\]
where $g(X_1, X_2, \hdots, X_n) = \frac{1}{N} \sum_{m=1}^N \sin(X_m \ln(X_m))$.

\end{document}