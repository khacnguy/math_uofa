\documentclass[11pt]{article}
    \title{\textbf{Math 217 Homework I}}
    \author{Khac Nguyen Nguyen}
    \date{}
    
    \addtolength{\topmargin}{-3cm}
    \addtolength{\textheight}{3cm}
    
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{xfrac}
\usepackage{hyperref}
\usepgfplotslibrary{polar}
\usepgflibrary{shapes.geometric}
\usetikzlibrary{calc}
\pgfplotsset{compat = newest}
\pgfplotsset{my style/.append style = {axis x line = middle, axis y line = middle, xlabel={$x$}, ylabel={$y$}, axis equal}}
\begin{document}
\tableofcontents
\pagebreak
\section{8 Sep}
\subsection{intuitive counting formula}
Let $N$ be a $\lambda$-Poisson process and $f$ be any continuous function in $\mathbb{R}$. 

We will prove that 
Note: $f(0) = f(N_0)$.
We have, 
\[
    f(N_1) - f(0) = \sum_{i=1}^{N_1} \left(f(i)-f(i-1) \right)
\]
By right contiuity of the poisson process 
\[
    \int_0^1 \left(f(N_{s^-} + 1) - f(N_{s^-})\right) dN_s = \lim_{M \to \infty} \left(f\left(N_{\frac{j-1}{M}}+1\right) - f\left(N_{\frac{j-1}{M}}\right) \right) \left(N_{\frac{j}{M}} - N_{\frac{j-1}{M}}\right)
\]
For large $M$, we have that 
\[
    N_{\frac{j}{M}} - N_{\frac{j-1}{M}} = 
    \begin{cases}
        1, \text{at the jump where $N$ increases} \\
        0, \text{otherwise}
    \end{cases}  
\] 
Hence, 
\[
    \int_0^1 \left(f(N_{s^-} + 1) - f(N_{s^-})\right) dN_s = \sum_{i=1}^{N_1} \left(f(i)-f(i-1) \right) = f(N_1) - f(0)
\]
\newpage
\subsection{problem}
\href{https://sbsprobability.com/b/43432?context=%5B"25842"%2C"m8"%2C"m8_1"%2C"43241"%5D}{problem link} \\
Guess
\[
    a_t = a_0 \exp \left({\int_0^1 f_s ds}\right)
\]
which starts right. The exponential rule gives 
\[
    \frac{d}{dt} a_t = f_t a_t    
\]
Unique? Consider second solution satisfies $\frac{d}{dt} \alpha_t = f_t \alpha_t$ such that $\alpha_0 = a_0$. 
Set $e_t = \frac{\alpha_t}{a_t}$, then $e_0 = \frac{a_0}{\alpha_0} = 1$ hence by product rule
\[
    \frac{d}{dt} e^t = \frac{f_t \alpha_t}{a_t} - \frac{\alpha_1 f_t \exp \left({-\int_0^1 f_s ds}\right)}{a_0} = 0
\]
Hence, $e_t = 1$, hence $\alpha_t = a_t$ for all $t$.
\section{11 Sep}
\subsection{HW8.1.2}  
Let $a_t = E[X_t]$ and $\beta = \lambda - \mu$. Then solve
\[
    a_t = a_0 + \int_0^t \beta a_s ds + \theta t     
\]
or 
\[
    \dot{a}_t - \beta a_t = \theta    
\]
Multiplying by
\begin{equation*}
    \begin{aligned}
            &I(t) = e^{-\beta t} \\
    &\frac{d}{ds} \left(I(s) a_s \right) = I(s) a_s - \underbrace{I(s)p}_{something}a_s = I(s) \theta
    \end{aligned}
\end{equation*}
and 
\[
    \int_0^t I(t) a_t -I(0)a_0 = \theta \int_0^t I(s) ds   
\]
so 
\[
    a_t = I^{-1}(t) (a_0 + \theta \int_0^t I(s)ds)    
\]
Solving 
\[
    a_t = e^{\beta t}a_0 + \theta e^{\beta t}(e^{-\beta t}-1) = e^{\beta t}a_0 + \frac{\theta}{\beta}(e^{\beta t }-1)
\]
Uniqueness \\
Let $\alpha_1$ be anothre solution
\[
    =alpha_t = \alpha_0 + \int_0^t \beta \alpha_s ds + \theta t     
\]
then 
\[
    e_t = \alpha_t - a_t    
\]
satisfies $ e_t = \int_0^t \beta e_s ds$ such that $e_0 = 0$ but $e_t = e^{\beta t}e_0 = 0$ is the unique solution.
\subsection{CR.11.1}
\subsection{HW8.1.3}
\[
    E[X_0] = \frac{1}{p} = 10    
\]
and 
\[
    E[X_0^2] = \frac{2-p}{p^2} = 190    
\]
Substitute $f(x) = x$ in, we get 
\[
    E[X_t]  = 10 + \int_0^t \frac{1}{10}E[X_s(X_s+1 + X_s - 1 -2X_s)]ds = 10 + \int_0^t 0ds = 10 
\]
and substitute $f(x) = x^2$ in, 
\begin{equation*}
    \begin{aligned}
        E[X_t^2] &= 190 + \int_0^t \frac{1}{10} E[X_s((X_s+1)^2 + (X_s-1)^2 -2X_s^2)] ds \\
        &= 190 + \int_0^t \frac{1}{10} E[2X_s] ds \\
        &= 190+2t
    \end{aligned}
\end{equation*}
Therefore, 
\[
    Var[X_t] = 90+2t
\]
\subsection{CR.11.4}
\section{15 Sep}
\subsection{Events and Marginals} 
Constraining Random vectors 
\[
    \{w:(x_1, \hdots , x_n, x_{n+1}) \in A \times \mathbb{R}\} = \{w:(x_1, \hdots , x_n, x_{n+1}) \subset A, \underbrace{x_{n+1} \in \mathbb{R}}_{\text{always true}} \} 
\]
\subsection{Generating $\sigma$-algebra}
Borel $\sigma$-algebra ar egenrated, which means smallest containing a class of sets. \\
Given a collection $\mathcal{C}$ of $\mathbb{R}$ or $\Omega$ say e.g. $\mathcal{C}_1 = \{ (-\infty, x): x \in \mathbb{R} \}$ and 
$\mathcal{C}_2 = \{ X^{-1}(-\infty, x): x \in \mathbb{R} \}$. Want the smallest $\sigma$-alg containing $\mathcal{C}_1$ or $\mathcal{C}_2$. \\

Note: $X^{-1}(-\infty, x) \subset \Omega \implies \mathcal{C}_2 \subset 2^\Omega. 
X^{-1}(-\infty, x] = \{w:X(w)\le x\}$

Note : $\sigma(\mathbb{R}) = \sigma{\mathcal{C}_1}$ and $\sigma(X) = \sigma{\mathcal{C}_2}$. \\
Definition: the $\sigma$-alg generated by $\mathcal{C}, \sigma{\mathcal{C}}$, is the smallest $\sigma$-alg containing 
$\mathcal{C}$ makes sense since
\begin{itemize}
    \item $2^\Omega$ is a $\sigma$-alg conataining $\mathcal{C}$
    \item If \{$\mathcal{F}_\alpha\}$ are $\sigma$-alg, then $\bigcup_\alpha \mathcal{F}_\alpha$ is also a $\sigma$-alg 
\end{itemize}
Step 1. Let \{$\mathcal{F}_\alpha\}$ be all $\sigma$-algs that contain $\mathcal{C}$. Not empty by fact 1. \\
Step 2. Let $\sigma(\mathcal{C}) = \bigcup_\alpha \mathcal{F}_\alpha$
\section{18 Sep}
\subsection{PP 8.8.1}
\subsection{HW 8.3.2}
Taking out knows
\begin{equation*}
    \begin{aligned}
        E[L_n | \mathcal{F}_{n-1}] &= E \left[ \left.\prod_{j=1}^n \frac{p(x_j)}{q(x_j)} \right| \mathcal{F}_{n-1}\right] \\
        &= \prod_{j-1}^{n-1} \frac{p(x_j)}{q(x_j)} E \left[ \left.\frac{p(x_)}{q(x_n)} \right| \mathcal{F}_{n-1} \right] \\
    \end{aligned}
\end{equation*}
But, by independence, 
\[
    E\left[ \left. \frac{p(x_n)}{q(x_n)} \right| \mathcal{F}_{n-1} \right] = E\left[ \frac{p(x_n)}{q(x_n)} \right] = \sum_{x \in A_x} \frac{p(x)}{q(x)} q(x) = 1
\]
Hence, $E[L_n|\mathcal{F}_{n-1}] = L_{n-1}$ and a $\{ \mathcal{F}_n \}$-martingale. \\
\begin{equation*}
    \begin{aligned}
        E[X_1X_3L_n] &= E[E[X_1X_3 L_n | \mathcal{F}_3]] \\
        &= E[X_1X_3E[L_n | \mathcal{F}_3]] \\
        &= E[X_1X_3L_3] \\
        &= E[E[X_1X_3L_3 | \mathcal{F}_2]] \\
        &= E\left[X_1L_2 E\left[ \left.X_3 \frac{p(X_3)}{q(X_3)}  \right| \mathcal{F}_2 \right]\right]
    \end{aligned}
\end{equation*}
But by independence, 
\begin{equation*}
    \begin{aligned}
        E\left[ \left. X_3 \frac{p(X_3)}{q(X_3)} \right| \mathcal{F}_2 \right] &= E\left[X_3 \frac{p(X_3)}{q(X_3)} \right] \\
        &= \sum_x x\frac{p(x)}{q(x)} q(x)\\
        &= \frac{1}{2}
    \end{aligned}
\end{equation*}
Similarly
\[
    E[E[X_1L_2|\mathcal{F}_1]] = E[E[X_1L_1|\mathcal{F}_1]] = E[X_1L_1] =  \sum_x x \frac{p(x)}{q(x)} q(x) = \frac{1}{2} 
\]
so $E[X_1X_3L_n] = \frac{1}{4}$.
\subsection{HW 8.2.1}
\subsection*{a.}
Trivially, it includes $\varnothing$
\subsection*{b.}
If $A \in 2^\Omega$ then $A$ is a subset of $\Omega$ and $A^C$ is a subset of $\Omega$ so $A^C \in 2^\Omega$
\subsection*{c.}
If $\{A_i\}_{i=1}^\infty \subset 2^\Omega$ then each $A_i$ is a subset of $\Omega$ and so is $\bigcup_i A_i$. 
Hence, $\bigcup_i A_i \in 2^\Omega$
\subsection{HW 8.2.5}
$\sigma(\mathbb{R})$ is defined as: \\
i. $(-\infty, x]  \in \sigma(\mathbb{R}) \forall x \in \mathbb{R}$ \\
ii. $\sigma(\mathbb{R})$ is a $\sigma$-algebra. \\
Basic sets $(-\infty, x]$ included so $(a,b] = (-\infty, b] \cap (-\infty, a]^C$ is also included. Hence, 
\[
    \left( \left.0,1-\frac{1}{n+1} \right] \right. \in \sigma(\mathbb{R}) \indent  \forall n\in \mathbb{N}   
\]
Hence, 
\[
    (0,1) = \bigcup_{n=1}^\infty \left( \left.0,1-\frac{1}{n+1} \right] \right. \in \sigma(\mathbb{R})
\]
\section{25 Sep}
\subsection{HW 8.4.2}
Suppose to contrary $P(Z >0)>0$. then $\exists \epsilon >0 $ such that 
\[
    P(Z>0)> \epsilon    
\]
But by continuous of measure
\[
    \lim_{n\to \infty} P(Z > 1/n) = P(\bigcup_{n=1}^\infty \{ Z > 1/n\}) = P(Z>0) > \epsilon    
\]
Hence, $\exists n >0$ such that 
\[
    P(Z>n) > \epsilon/2
\]
and 
\[
    E[Z 1_{Z>0}] > \frac{n\epsilon}{2} > 0    
\]
Proved hint.
Suppose 2 solutions: $Y,Z$. Then 
\begin{equation*}
    \begin{aligned}
        0 &= E[X 1_G] - E[X1_G] \\
        &= E[Y 1_G] - E[Z 1_G] \\
        &= E[(Y-Z)1_G]
    \end{aligned}
\end{equation*}
but $G = \{ Y-Z>0\}$ and $G = \{ Y-Z < 0\}$ are in $\mathcal{F}$ so 
\[
    E[(Y-Z) 1_{Y-Z > 0}] = 0 = E[(Y-Z)1_{Y-Z<0}]    
\]
and $Y=Z$ by hint.
\subsection{PP 8.5.1}
\subsection{HW 8.4.3}
\[
    E[X|Y] = \sum_y \underbrace{E[X|Y =y]}_{\text{value it takes}} \underbrace{1_{y=Y}}_{\text{where takes}}    
\]
and $\sigma(y)$ is the collection of unions of $\{ w: Y(w) = y\}$ for distinct $y \in A_y$
Have to show 3 properties of cond. exp.
\subsubsection*{i.  $E[\left|E[X|Y]\right|] \le E[E[\left|X\right||Y]] = E[|X|] < \infty $}
\subsubsection*{ii. Suppose $B \in \sigma{\mathbb{R}}$. Then $E[X|Y]^{-1}(B) = \bigcup_{y: E[X|Y] \in B} \{w: Y{w} = y\} \in \sigma(Y)$}
\subsection*{iii. Suppose $G \in \sigma(y)$. Then $G = Y^{-1}(B)$ for $B \in \sigma(\mathbb{R}$)}
\begin{equation*}
    \begin{aligned}
        E[X 1_G] &= E[X 1_B(Y)] = E[X 1_{\bigcup_{y \in B} \{Y = y\}}] \\
        &= \sum_{y \in B} E[X 1_{\{Y = y\}}] \\
        &= \sum_{y \in B} \sum_{x,y} x1_{z = y} \underbrace{P_{xy}(x,y)}_{P_{x|y}(x|z) P_y(x)} \\
        &= \sum_{y \in B} \underbrace{\sum_x x P_{x|y}(x|y)}_{E[X|Y=y]} P_y(y) \\
        &= \sum_{y \in A_y} E[X |Y=y] 1_B{y} P_y(y) \\
        &= E[E[X|Y] \underbrace{1_B(Y)}_{1_G}]
    \end{aligned}
\end{equation*}
\subsection{HW 8.5.2}
Let $N^+, N^-$ be independent PP with rates $\frac{N^2}{2}$. Take $X_t = \frac{N^+ - N^-}{N}$. Then, by MP for PP
and role of independent.
\begin{equation*}
    \begin{aligned}
        f(X_t) &= f\left( \frac{N_t^+ - N_t^-}{N} \right) = g(N_t^+, N_t^-) \\
        &= \int_0^t \underbrace{\frac{N^2}{2}}_\lambda \left( f\left( \frac{N_u^+ + 1 - N_u^-}{N} \right) - 2f\left( \frac{N_u^+ - N_u^-}{N} \right) + f\left( \frac{N_u^+ -1 - N_u^-}{N}\right)\right)
        &= \int_0^t \underbrace{\frac{N^2}{2}}_\lambda \left( f\left( X_u + \frac{1}{N}\right) - 2f\left( \frac{X_u}{N} \right) + f\left( X_u - \frac{1}{N}\right)\right)
    \end{aligned}
\end{equation*}
\section{2 Oct}
\subsection{HW 8.7.2}
MP for $B$ is 
\[
    f(B_t) - f(0) - \int_9^t \frac{1}{2} f''(B_s) ds 
\]
IF $f(x) = g(x^3)$, then by chain rule
\[
    g''(x) = \frac{d}{dx}3x^2 q'(x^3) = 6xq'(x^3) + 9x^4 q''(x^3)    
\]
and hence
\[
    g(B_t)^3 - g(0) - \int_0^t \frac{1}{2} g''(B_s)ds = g(B_t^3)  - g(0) - \int_0^t 3X_s^{1/3} g'(X_3) + \frac{9}{2} X_s^{4/3}q''(X_s)ds
\] 
So 
\[
    Lg(x) = 3x^{1/3}q'(x) + \frac{9}{2}x^{4/3} q''(x)    
\]
and SDE is 
\[
    dX_t = 3X_t^{1/3} + 3X_t^{2/3} dB_t    
\]
\subsection{PP 8.7.7}
\subsection{PP 8.6.7}
\subsection{PP 8.6.8}
\end{document}